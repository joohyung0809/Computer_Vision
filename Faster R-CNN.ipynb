{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNH1FFg2MeGjbwTcnQqd8x8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joohyung0809/Computer_Vision/blob/main/Faster%20R-CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ZkwqabHwBM",
        "outputId": "3d7fe758-88f4-4300-cdcc-b59f9a501f52"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchnet in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchnet) (2.0.1+cu118)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchnet) (1.16.0)\n",
            "Requirement already satisfied: visdom in /usr/local/lib/python3.10/dist-packages (from torchnet) (0.2.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchnet) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchnet) (16.0.5)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (2.27.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (6.3.1)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.32)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.5.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchnet) (2.1.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch->visdom->torchnet) (2.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchnet) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "_EWVOcEbAQQY"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "import os\n",
        "import six\n",
        "from collections import namedtuple\n",
        "\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "# COCO formate의 데이터셋 사용을 돕는 라이브러리\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# torchvision - computer vision용 pytorch 라이브러리\n",
        "from torchvision.models import vgg16\n",
        "from torchvision.ops import RoIPool\n",
        "from torchvision.ops import nms\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils import data as data_\n",
        "\n",
        "# torchnet - logging, eval, visualize 등을 돕는 라이브러리\n",
        "from torchnet.meter import ConfusionMeter, AverageValueMeter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 하이퍼 파라미터 세팅"
      ],
      "metadata": {
        "id": "E_iz5hH4BfRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 불러오기"
      ],
      "metadata": {
        "id": "QiaR-vKBCw7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TrainDataset\n",
        "class TrainCustom(Dataset):\n",
        "    def __init__(self, annotation, data_dir, transforms = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            annotation: annotation 파일 위치\n",
        "            data_dir: data가 존재하는 폴더 경로\n",
        "            transforms : transform or not\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        # coco annotation 불러오기 (coco API)\n",
        "        self.coco = COCO(annotation)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        \n",
        "        # 이미지 아이디 가져오기\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "\n",
        "        # 이미지 정보 가져오기\n",
        "        image_info = self.coco.loadImgs(image_id)[0]\n",
        "\n",
        "        # 이미지 로드\n",
        "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "\n",
        "        # 어노테이션 파일 로드\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # 박스 가져오기\n",
        "        boxes = np.array([x['bbox'] for x in anns])\n",
        "\n",
        "        # boxes (x_min, y_min, x_max, y_max) -> 이 형태로 만들어주기\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
        "\n",
        "        # 레이블 가져오기\n",
        "        labels = np.array([x['category_id'] for x in anns])\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # transform 함수 정의\n",
        "        if self.transforms :\n",
        "            scale = 1.0  # resize scale\n",
        "            H, W, _ = image.shape\n",
        "            resize_H = int(scale * H)\n",
        "            resize_W = int(scale * W)\n",
        "            transforms = get_train_transform(resize_H, resize_W)\n",
        "        else :\n",
        "            scale = 1.0\n",
        "            transforms = no_transform()\n",
        "        \n",
        "        # transform\n",
        "        sample = {\n",
        "            'image': image,\n",
        "            'bboxes': boxes,\n",
        "            'labels': labels\n",
        "        }\n",
        "        sample = transforms(**sample)\n",
        "        image = sample['image']\n",
        "        bboxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
        "        boxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
        "\n",
        "        # bboxes (x_min, y_min, x_max, y_max) -> boxes (y_min, x_min, y_max, x_max) 계산의 편의를 위해\n",
        "        boxes[:, 0] = bboxes[:, 1]\n",
        "        boxes[:, 1] = bboxes[:, 0]\n",
        "        boxes[:, 2] = bboxes[:, 3]\n",
        "        boxes[:, 3] = bboxes[:, 2]\n",
        "\n",
        "        return image, boxes, labels, scale\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.coco.getImgIds())\n",
        "\n",
        "# Test Datset\n",
        "class TestCustom(Dataset):\n",
        "    def __init__(self, annotation, data_dir):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            annotation: annotation 파일 위치\n",
        "            data_dir: data가 존재하는 폴더 경로\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        # coco annotation 불러오기 (coco API)\n",
        "        self.coco = COCO(annotation)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        \n",
        "        # 이미지 아이디 가져오기\n",
        "        image_id = self.coco.getImgIds(imgIds=index)\n",
        "\n",
        "        # 이미지 정보 가져오기\n",
        "        image_info = self.coco.loadImgs(image_id)[0]\n",
        "\n",
        "        # 이미지 로드\n",
        "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "        image = torch.tensor(image, dtype = torch.float).permute(2,0,1)\n",
        "        \n",
        "        return image, image.shape[1:]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.coco.getImgIds())\n"
      ],
      "metadata": {
        "id": "z6ded0JaCvLv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- transform"
      ],
      "metadata": {
        "id": "s_tafAJBJC3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train dataset transform\n",
        "def get_train_transform(h, w):\n",
        "    return A.Compose([\n",
        "        A.Resize(height = h, width = w),\n",
        "        A.Flip(p=0.5),\n",
        "        ToTensorV2(p=1.0)\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "# No transform\n",
        "def no_transform():\n",
        "    return A.Compose([\n",
        "        ToTensorV2(p=1.0) # format for pytorch tensor\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "metadata": {
        "id": "JUu8t-cAHnAM"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- backbone 생성"
      ],
      "metadata": {
        "id": "TeK1P_zLJWRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decom_vgg16():\n",
        "    # the 30th layer of features is relu of conv5_3\n",
        "    model = vgg16(pretrained=True)\n",
        "    \n",
        "    features = list(model.features)[:30]\n",
        "    classifier = model.classifier\n",
        "\n",
        "    classifier = list(classifier)\n",
        "    del classifier[6]\n",
        "    if not use_drop:\n",
        "        del classifier[5]\n",
        "        del classifier[2]\n",
        "    classifier = nn.Sequential(*classifier)\n",
        "\n",
        "    # freeze top4 conv\n",
        "    for layer in features[:10]:\n",
        "        for p in layer.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    return nn.Sequential(*features), classifier # 얘로 feature map 추출"
      ],
      "metadata": {
        "id": "hPCgGtsOJFPI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위에서 생긴 feature map을 RPN에 통과시켜야 함\n",
        "## RPN 정의\n",
        "1. Anchor box 생성\n",
        "2. Proposal 생성\n",
        "  - RPN에서 구한 rpn_loc와 anchor를 통해서 ROI를 생성\n",
        "  - ROI 개수 줄이기 위해 미리 정해둔 크기(min_size) 에 맞는 ROI들 중에서 NMS를 통해 최종 ROI 반환(train시 2000개)\n",
        "3. Region Proposal Network\n",
        "  - VGG16 통과한 feature map으로부터 region proposal들 생성"
      ],
      "metadata": {
        "id": "YQmtuns7JiCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Anchor box 생성(한 픽셀 당 몇 개를 생성할 거냐 정하는 단계)\n",
        "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]):\n",
        "    \"\"\" \n",
        "    Args:\n",
        "        ratios: 비율\n",
        "        anchor_scales: 스케일\n",
        "    Returns: basic anchor boxes, shape=(R, 4)\n",
        "        R: len(ratio) * len(anchor_scales) = anchor 개수 = 9\n",
        "        4: anchor box 좌표 값\n",
        "    \"\"\"\n",
        "\n",
        "    py = base_size / 2. # center y\n",
        "    px = base_size / 2. # center x\n",
        "\n",
        "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32) # anchor_box\n",
        "    \n",
        "    for i in six.moves.range(len(ratios)):\n",
        "        for j in six.moves.range(len(anchor_scales)):\n",
        "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n",
        "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n",
        "\n",
        "            index = i * len(anchor_scales) + j\n",
        "            # offset of anchor box\n",
        "            anchor_base[index, 0] = py - h / 2. # y_min\n",
        "            anchor_base[index, 1] = px - w / 2. # x_min\n",
        "            anchor_base[index, 2] = py + h / 2. # y_max\n",
        "            anchor_base[index, 3] = px + w / 2. # x_max\n",
        "            \n",
        "    return anchor_base # (9,4)\n",
        "    # (9,4) 짜리 Anchor box 생성"
      ],
      "metadata": {
        "id": "kTXR7ShFJaAO"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Score 정보 얻기\n",
        "class ProposalCreator:\n",
        "    def __init__(self, parent_model,\n",
        "                 nms_thresh=0.7, # nms threshold\n",
        "                 n_train_pre_nms=12000, # train시 nms 전 roi 개수\n",
        "                 n_train_post_nms=2000, # train시 nms 후 roi 개수\n",
        "                 n_test_pre_nms=6000,   # test시 nms 전 roi 개수\n",
        "                 n_test_post_nms=300,   # test시 nms 후 roi 개수\n",
        "                 min_size=16            \n",
        "                 ):\n",
        "        self.parent_model = parent_model # 해당 모델이 train중인지 test중인지 나타냄\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.n_train_pre_nms = n_train_pre_nms\n",
        "        self.n_train_post_nms = n_train_post_nms\n",
        "        self.n_test_pre_nms = n_test_pre_nms\n",
        "        self.n_test_post_nms = n_test_post_nms\n",
        "        self.min_size = min_size\n",
        "\n",
        "    def __call__(self, loc, score, anchor, img_size, scale=1.):    \n",
        "        if self.parent_model.training: # train중일 때\n",
        "            n_pre_nms = self.n_train_pre_nms\n",
        "            n_post_nms = self.n_train_post_nms\n",
        "        else: # test중일 때\n",
        "            n_pre_nms = self.n_test_pre_nms\n",
        "            n_post_nms = self.n_test_post_nms\n",
        "\n",
        "        # 미세조정 하는 구간\n",
        "        roi = loc2bbox(anchor, loc) # anchor의 좌표값과 predicted bounding bounding box offset(y,x,h,w)를 통해 bounding box 좌표값(y_min, x_min, y_max, x_max) 생성\n",
        "\n",
        "        # Clip predicted boxes to image.\n",
        "        roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
        "        roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
        "\n",
        "        # min_size 보다 작은 box들은 제거 (잘라내는 과정)\n",
        "        min_size = self.min_size * scale\n",
        "        hs = roi[:, 2] - roi[:, 0]\n",
        "        ws = roi[:, 3] - roi[:, 1]\n",
        "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
        "        roi = roi[keep, :]\n",
        "        score = score[keep]\n",
        "        \n",
        "        # Sort all (proposal, score) pairs by score from highest to lowest.\n",
        "        # Take top pre_nms_topN \n",
        "        order = score.ravel().argsort()[::-1]\n",
        "        if n_pre_nms > 0:\n",
        "            order = order[:n_pre_nms]\n",
        "        roi = roi[order, :]\n",
        "        score = score[order]\n",
        "\n",
        "        # nms 적용\n",
        "        keep = nms(\n",
        "            torch.from_numpy(roi).cuda(),\n",
        "            torch.from_numpy(score).cuda(),\n",
        "            self.nms_thresh)\n",
        "        if n_post_nms > 0:\n",
        "            keep = keep[:n_post_nms]\n",
        "        roi = roi[keep.cpu().numpy()]\n",
        "        \n",
        "        return roi "
      ],
      "metadata": {
        "id": "d2JE0-UALop9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RegionProposalNetwork(nn.Module):\n",
        "    def __init__(self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
        "                 anchor_scales=[8, 16, 32], feat_stride=16, proposal_creator_params=dict(),):\n",
        "        \n",
        "        super(RegionProposalNetwork, self).__init__()\n",
        "\n",
        "        self.anchor_base = generate_anchor_base(anchor_scales=anchor_scales, ratios=ratios) # 9개의 anchorbox 생성\n",
        "        self.feat_stride = feat_stride\n",
        "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params) # proposal_creator_params : 해당 네트워크가 training인지 testing인지 알려준다.\n",
        "        n_anchor = self.anchor_base.shape[0] # anchor 개수\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
        "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)  # 9*2\n",
        "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)   # 9*4\n",
        "        normal_init(self.conv1, 0, 0.01) # weight initalizer\n",
        "        normal_init(self.score, 0, 0.01) # weight initalizer\n",
        "        normal_init(self.loc, 0, 0.01)   # weight initalizer\n",
        "\n",
        "    def forward(self, x, img_size, scale=1.):\n",
        "        # x(feature map)\n",
        "        n, _, hh, ww = x.shape\n",
        "\n",
        "        # 전체 (h*w*9)개 anchor의 좌표값 # anchor_base:(9, 4)\n",
        "        anchor = _enumerate_shifted_anchor(np.array(self.anchor_base), self.feat_stride, hh, ww) #  base anchor box를 픽셀 전체로 확장\n",
        "        # anchor (9216, 4)\n",
        "        n_anchor = anchor.shape[0] // (hh * ww) # anchor 개수\n",
        "        \n",
        "        middle = F.relu(self.conv1(x))\n",
        "        \n",
        "        # anchor box에 맞게 밑에 모두 reshape\n",
        "\n",
        "        # predicted bounding box offset\n",
        "        rpn_locs = self.loc(middle) # torch.Size([1, 36, 32, 32])\n",
        "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4) # torch.Size([1, 9216, 4]) -> 얼마나 미세조정 해줘야 되냐 정보\n",
        "\n",
        "        # predicted scores for anchor (foreground or background)\n",
        "        rpn_scores = self.score(middle)  # torch.Size([1, 18, 32, 32])\n",
        "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous() # torch.Size([1, 32, 32, 18])\n",
        "        \n",
        "        # scores for foreground\n",
        "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4) # torch.Size([1, 32, 32, 9, 2])\n",
        "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous() # torch.Size([1, 32, 32, 9])\n",
        "        rpn_fg_scores = rpn_fg_scores.view(n, -1) # torch.Size([1, 9216])\n",
        "        \n",
        "        rpn_scores = rpn_scores.view(n, -1, 2) # torch.Size([1, 9216, 2]) -> 얼마나 score가 나왔냐 정보\n",
        "\n",
        "        # proposal생성 (ProposalCreator)\n",
        "        rois = list()        # proposal의 좌표값이 있는 bounding box array\n",
        "        roi_indices = list() # roi에 해당하는 image 인덱스\n",
        "        for i in range(n):\n",
        "            # proposal_layer 돌리면 2000개의 ROI가 나옴\n",
        "            roi = self.proposal_layer(rpn_locs[i].cpu().data.numpy(),rpn_fg_scores[i].cpu().data.numpy(),anchor, img_size,scale=scale) \n",
        "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
        "            rois.append(roi)\n",
        "            roi_indices.append(batch_index)\n",
        "        rois = np.concatenate(rois, axis=0)\n",
        "        roi_indices = np.concatenate(roi_indices, axis=0)\n",
        "        \n",
        "        return rpn_locs, rpn_scores, rois, roi_indices, anchor # 2000개의 ROI를 묶어서 return 하는 형태\n",
        "\n",
        "\n",
        "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
        "    # anchor_base는 하나의 pixel에 9개 종류의 anchor box를 나타냄\n",
        "    # 이것을 enumerate시켜 전체 이미지의 pixel에 각각 9개의 anchor box를 가지게 함\n",
        "    # 32x32 feature map에서는 32x32x9=9216개 anchor box가짐\n",
        "\n",
        "    shift_y = np.arange(0, height * feat_stride, feat_stride)\n",
        "    shift_x = np.arange(0, width * feat_stride, feat_stride)\n",
        "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
        "    shift = np.stack((shift_y.ravel(), shift_x.ravel(),\n",
        "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
        "\n",
        "    A = anchor_base.shape[0]\n",
        "    K = shift.shape[0]\n",
        "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
        "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
        "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
        "    return anchor # (9216, 4)\n"
      ],
      "metadata": {
        "id": "adNX26JBMidU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faster R-CNN head 정의\n",
        "- ROI pool 후에 clasifier, regression 통과\n",
        "#### 다시 말하면 최종 목표는 <br> RPN으로 부터 나온 ROI받아서 projection으로  Pooling하고 <br> 그로부터 나온 고정된 feature vector로 부터 class 예측, 정확한 box예측하는 것"
      ],
      "metadata": {
        "id": "Bt_vjouwRaaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16RoIHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Faster R-CNN head\n",
        "    RoI pool 후에 classifier, regressior 통과\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_class, roi_size, spatial_scale, classifier):\n",
        "        super(VGG16RoIHead, self).__init__()\n",
        "\n",
        "        self.classifier = classifier  \n",
        "        self.cls_loc = nn.Linear(4096, n_class * 4) # bounding box regressor\n",
        "        self.score = nn.Linear(4096, n_class) # Classifier\n",
        "\n",
        "        normal_init(self.cls_loc, 0, 0.001)  # weight initialize\n",
        "        normal_init(self.score, 0, 0.01)     # weight initialize\n",
        "\n",
        "        self.n_class = n_class # 배경 포함한 class 수\n",
        "        self.roi_size = roi_size # RoI-pooling 후 feature map의  높이, 너비 (참고로 ROI size는 target size)\n",
        "        self.spatial_scale = spatial_scale # roi resize scale\n",
        "        self.roi = RoIPool( (self.roi_size, self.roi_size),self.spatial_scale) # torchvision에서 제공해줌 -> projection까지 제공\n",
        "\n",
        "    def forward(self, x, rois, roi_indices):\n",
        "        # in case roi_indices is  ndarray\n",
        "        roi_indices = totensor(roi_indices).float()\n",
        "        rois = totensor(rois).float()\n",
        "        indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
        "        # NOTE: important: yx->xy\n",
        "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
        "        indices_and_rois =  xy_indices_and_rois.contiguous() \n",
        "\n",
        "        # 각 이미지 roi pooling \n",
        "        pool = self.roi(x, indices_and_rois) \n",
        "        # flatten \n",
        "        pool = pool.view(pool.size(0), -1)\n",
        "        # fully connected\n",
        "        fc7 = self.classifier(pool)\n",
        "        # regression \n",
        "        roi_cls_locs = self.cls_loc(fc7)\n",
        "        # softmax\n",
        "        roi_scores = self.score(fc7)\n",
        "\n",
        "        \n",
        "        return roi_cls_locs, roi_scores"
      ],
      "metadata": {
        "id": "3XFBmoWCNKPG"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Faster R-CNN 정의\n",
        "- 지금까지 선언한 걸 통합하면 됨\n",
        "- Feature Extraction : image로부터 feature map 생성\n",
        "- Region Proposal Networks : Region of Interset 생성\n",
        "- Localization and Classification Head : ROI에 해당하는 feature map을 최종 detect"
      ],
      "metadata": {
        "id": "lNejWAe1S5ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nograd(f):\n",
        "    def new_f(*args, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            return f(*args, **kwargs)\n",
        "    return new_f\n",
        "\n",
        "class FasterRCNN(nn.Module):\n",
        "    def __init__(self, extractor, rpn, head,\n",
        "                loc_normalize_mean = (0., 0., 0., 0.),\n",
        "                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.extractor = extractor  # extractor : vgg\n",
        "        self.rpn = rpn              # rpn : region proposal network\n",
        "        self.head = head            # head : RoiHead\n",
        "\n",
        "        # mean and std\n",
        "        self.loc_normalize_mean = loc_normalize_mean\n",
        "        self.loc_normalize_std = loc_normalize_std\n",
        "        self.use_preset()\n",
        "\n",
        "    @property\n",
        "    def n_class(self): # 최종 class 개수 (배경 포함)\n",
        "        return self.head.n_class\n",
        "\n",
        "    # predict 시 사용하는 forward\n",
        "    # train 시 FasterRCNNTrainer을 사용하여 FasterRcnn에 있는 extractor, rpn, head를 모듈별로 불러와서 forward\n",
        "    def forward(self, x, scale=1.):\n",
        "        img_size = x.shape[2:]\n",
        "\n",
        "        h = self.extractor(x) # extractor 통과\n",
        "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.rpn(h, img_size, scale) # rpn 통과\n",
        "        roi_cls_locs, roi_scores = self.head(h, rois, roi_indices) # head 통과\n",
        "        return roi_cls_locs, roi_scores, rois, roi_indices \n",
        "\n",
        "    def use_preset(self): # prediction 과정 쓰이는 threshold 정의\n",
        "        self.nms_thresh = 0.3\n",
        "        self.score_thresh = 0.05\n",
        "\n",
        "    def _suppress(self, raw_cls_bbox, raw_prob):\n",
        "        bbox = list()\n",
        "        label = list()\n",
        "        score = list()\n",
        "        \n",
        "        # skip cls_id = 0 because it is the background class\n",
        "        for l in range(1, self.n_class):\n",
        "            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n",
        "            prob_l = raw_prob[:, l]\n",
        "            mask = prob_l > self.score_thresh\n",
        "            cls_bbox_l = cls_bbox_l[mask]\n",
        "            prob_l = prob_l[mask]\n",
        "            keep = nms(cls_bbox_l, prob_l,self.nms_thresh)\n",
        "            bbox.append(cls_bbox_l[keep].cpu().numpy())\n",
        "            # The labels are in [0, self.n_class - 2].\n",
        "            label.append((l - 1) * np.ones((len(keep),)))\n",
        "            score.append(prob_l[keep].cpu().numpy())\n",
        "        \n",
        "        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n",
        "        label = np.concatenate(label, axis=0).astype(np.int32)\n",
        "        score = np.concatenate(score, axis=0).astype(np.float32)\n",
        "        return bbox, label, score\n",
        "\n",
        "    @nograd\n",
        "    def predict(self, imgs,sizes=None):\n",
        "        \"\"\"\n",
        "        이미지에서 객체 검출\n",
        "        Input : images\n",
        "        Output : bboxes, labels, scores\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        prepared_imgs = imgs\n",
        "                \n",
        "        bboxes = list()\n",
        "        labels = list()\n",
        "        scores = list()\n",
        "        for img, size in zip(prepared_imgs, sizes):\n",
        "            img = totensor(img[None]).float()\n",
        "            scale = img.shape[3] / size[1]\n",
        "            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale) # self = FasterRCNN\n",
        "            # We are assuming that batch size is 1.\n",
        "            roi_score = roi_scores.data\n",
        "            roi_cls_loc = roi_cls_loc.data\n",
        "            roi = totensor(rois) / scale\n",
        "\n",
        "            # Convert predictions to bounding boxes in image coordinates.\n",
        "            # Bounding boxes are scaled to the scale of the input images.\n",
        "            mean = torch.Tensor(self.loc_normalize_mean).cuda(). repeat(self.n_class)[None]\n",
        "            std = torch.Tensor(self.loc_normalize_std).cuda(). repeat(self.n_class)[None]\n",
        "\n",
        "            roi_cls_loc = (roi_cls_loc * std + mean)\n",
        "            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n",
        "            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n",
        "            cls_bbox = loc2bbox(tonumpy(roi).reshape((-1, 4)),tonumpy(roi_cls_loc).reshape((-1, 4)))\n",
        "            cls_bbox = totensor(cls_bbox)\n",
        "            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n",
        "            # clip bounding box\n",
        "            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n",
        "            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n",
        "\n",
        "            prob = (F.softmax(totensor(roi_score), dim=1))\n",
        "\n",
        "            bbox, label, score = self._suppress(cls_bbox, prob)\n",
        "            bboxes.append(bbox)\n",
        "            labels.append(label)\n",
        "            scores.append(score)\n",
        "\n",
        "        self.use_preset()\n",
        "        self.train()\n",
        "        return bboxes, labels, scores\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        '''\n",
        "        Optimizer 선언\n",
        "        '''\n",
        "        lr = learning_rate\n",
        "        params = []\n",
        "        for key, value in dict(self.named_parameters()).items():\n",
        "            if value.requires_grad:\n",
        "                if 'bias' in key:\n",
        "                    params += [{'params': [value], 'lr': lr * 2, 'weight_decay': 0}]\n",
        "                else:\n",
        "                    params += [{'params': [value], 'lr': lr, 'weight_decay': weight_decay}]\n",
        "        self.optimizer = torch.optim.SGD(params, momentum=0.9)\n",
        "        return self.optimizer\n",
        "\n",
        "    def scale_lr(self, decay=0.1):\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] *= decay\n",
        "        return self.optimizer\n"
      ],
      "metadata": {
        "id": "dt8itmdPS20O"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faster R-CNN 생성\n",
        "\n"
      ],
      "metadata": {
        "id": "uEockNniV2g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNNVGG16(FasterRCNN): # backbone으로 VGG16 사용했음\n",
        "\n",
        "    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n",
        "\n",
        "    def __init__(self, n_fg_class=10, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32] ): # n_fg_class : 배경포함 하지 않은 class 개수        \n",
        "        extractor, classifier = decom_vgg16()\n",
        "        \n",
        "        rpn = RegionProposalNetwork(\n",
        "            512, 512,\n",
        "            ratios=ratios,\n",
        "            anchor_scales=anchor_scales,\n",
        "            feat_stride=self.feat_stride,\n",
        "        )\n",
        "\n",
        "        head = VGG16RoIHead(\n",
        "            n_class=n_fg_class + 1,\n",
        "            roi_size=7,\n",
        "            spatial_scale=(1. / self.feat_stride),\n",
        "            classifier=classifier\n",
        "        )\n",
        "        super(FasterRCNNVGG16, self).__init__( # fast RCNN 상속 받았으니 super를 통해 넣어줌\n",
        "            extractor,\n",
        "            rpn,\n",
        "            head,\n",
        "        )"
      ],
      "metadata": {
        "id": "Muw-3ipkTscJ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- trainer 정의\n",
        "  - 지금까지는 모델만 정의\n",
        "  - 우린 RPN, ROI head를 훈련시켜야 함\n",
        "\n",
        "## 그 애들이\n",
        "1. Anchor Target Creator\n",
        "  - Anchor box에 해당하는 ground truth bounding box match\n",
        "  - Region Proposal Network loss 구할 때 ground truth로 사용\n",
        "2. positive, negative sampling(Proposal target Creator)\n",
        "  - RPN에서 NMS를 거친 roi들을 ground truth와의 IOU비교\n",
        "  - Positive/Negative sampling 수행(총 128개)\n",
        "  - sample ROI와 gt_bbox regression에서 regression 해야할 ground truth loc값(t_x, t_y, t_w, t_h) 구함\n",
        "\n"
      ],
      "metadata": {
        "id": "pkGkiFF7UAIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. util 함수 정의"
      ],
      "metadata": {
        "id": "HoSBP8IyVqI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bbox_iou(bbox_a, bbox_b):\n",
        "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
        "        raise IndexError\n",
        "\n",
        "    #bbox_a 1개와 bbox_b k개를 비교해야하므로 None을 이용해서 차원을 늘려서 연산한다.\n",
        "    # top left\n",
        "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
        "    # bottom right\n",
        "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
        "\n",
        "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
        "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
        "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
        "    return area_i / (area_a[:, None] + area_b - area_i)"
      ],
      "metadata": {
        "id": "kkndkhF3T_bR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Anchor Target Creator\n"
      ],
      "metadata": {
        "id": "HIv8W6EEWHFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnchorTargetCreator(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_sample=256,\n",
        "                 pos_iou_thresh=0.7, neg_iou_thresh=0.3,\n",
        "                 pos_ratio=0.5):\n",
        "        self.n_sample = n_sample\n",
        "        self.pos_iou_thresh = pos_iou_thresh\n",
        "        self.neg_iou_thresh = neg_iou_thresh\n",
        "        self.pos_ratio = pos_ratio\n",
        "\n",
        "    def __call__(self, bbox, anchor, img_size):\n",
        "\n",
        "        img_H, img_W = img_size\n",
        "\n",
        "        n_anchor = len(anchor) # 9216\n",
        "        inside_index = get_inside_index(anchor, img_H, img_W) # (2272,) -> 안쪽에 위치하는 box만을 catch해서 그것의 index만 가져온다.\n",
        "        anchor = anchor[inside_index] # (2272, 4)\n",
        "        argmax_ious, label = self._create_label(\n",
        "            inside_index, anchor, bbox)\n",
        "\n",
        "        # compute bounding box regression targets\n",
        "        loc = bbox2loc(anchor, bbox[argmax_ious]) # (2272, 4)\n",
        "\n",
        "        # map up to original set of anchors\n",
        "        label = unmap(label, n_anchor, inside_index, fill=-1) # (9216,)\n",
        "        loc = unmap(loc, n_anchor, inside_index, fill=0) # (9216, 4)\n",
        "\n",
        "        return loc, label\n",
        "\n",
        "    def _create_label(self, inside_index, anchor, bbox):\n",
        "        # Positive / Negative sample을 만들어내는 과정\n",
        "        # label) 1 :positive, 0 : negative, -1 : dont care\n",
        "        label = np.empty((len(inside_index),), dtype=np.int32)\n",
        "        label.fill(-1)\n",
        "\n",
        "        argmax_ious, max_ious, gt_argmax_ious = self._calc_ious(anchor, bbox, inside_index)\n",
        "\n",
        "        label[max_ious < self.neg_iou_thresh] = 0 # 0.3 이하는 negative\n",
        "\n",
        "        # 가장 iou가 큰 것은 positive label\n",
        "        label[gt_argmax_ious] = 1\n",
        "\n",
        "        # positive label\n",
        "        label[max_ious >= self.pos_iou_thresh] = 1 # 0.7\n",
        "\n",
        "        # subsample positive labels if we have too many\n",
        "        n_pos = int(self.pos_ratio * self.n_sample)\n",
        "        pos_index = np.where(label == 1)[0]\n",
        "        if len(pos_index) > n_pos:\n",
        "            disable_index = np.random.choice(\n",
        "                pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
        "            label[disable_index] = -1\n",
        "\n",
        "        # subsample negative labels if we have too many\n",
        "        n_neg = self.n_sample - np.sum(label == 1)\n",
        "        neg_index = np.where(label == 0)[0]\n",
        "        if len(neg_index) > n_neg:\n",
        "            disable_index = np.random.choice(\n",
        "                neg_index, size=(len(neg_index) - n_neg), replace=False)\n",
        "            label[disable_index] = -1\n",
        "\n",
        "        return argmax_ious, label\n",
        "\n",
        "    def _calc_ious(self, anchor, bbox, inside_index):\n",
        "        # ious between the anchors and the gt boxes\n",
        "        ious = bbox_iou(anchor, bbox)\n",
        "        argmax_ious = ious.argmax(axis=1)\n",
        "        max_ious = ious[np.arange(len(inside_index)), argmax_ious]\n",
        "        gt_argmax_ious = ious.argmax(axis=0)\n",
        "        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
        "        gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
        "\n",
        "        return argmax_ious, max_ious, gt_argmax_ious"
      ],
      "metadata": {
        "id": "EGIeOGiuWKjt"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. positive, negative sampling(Proposal target Creator)\n",
        "- ROI들을 또 Positve / Negative 나누어야 함"
      ],
      "metadata": {
        "id": "VxvYtubccZ1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProposalTargetCreator:\n",
        "    def __init__(self,\n",
        "                 n_sample=128,\n",
        "                 pos_ratio=0.25, pos_iou_thresh=0.5,\n",
        "                 neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0\n",
        "                 ):\n",
        "        self.n_sample = n_sample\n",
        "        self.pos_ratio = pos_ratio\n",
        "        self.pos_iou_thresh = pos_iou_thresh # positive iou threshold\n",
        "        self.neg_iou_thresh_hi = neg_iou_thresh_hi # negitave iou threshold = (neg_iou_thresh_hi ~ neg_iou_thresh_lo)\n",
        "        self.neg_iou_thresh_lo = neg_iou_thresh_lo \n",
        "\n",
        "    def __call__(self, roi, bbox, label,\n",
        "                 loc_normalize_mean=(0., 0., 0., 0.),\n",
        "                 loc_normalize_std=(0.1, 0.1, 0.2, 0.2)):\n",
        "        n_bbox, _ = bbox.shape\n",
        "\n",
        "        roi = np.concatenate((roi, bbox), axis=0)\n",
        "\n",
        "        pos_roi_per_image = np.round(self.n_sample * self.pos_ratio) # positive image 갯수 = 32\n",
        "        iou = bbox_iou(roi, bbox) # RoI와 bounding box IoU\n",
        "        gt_assignment = iou.argmax(axis=1)\n",
        "        max_iou = iou.max(axis=1)\n",
        "        gt_roi_label = label[gt_assignment] + 1 # class label [0, n_fg_class - 1] -> [1, n_fg_class].\n",
        "\n",
        "        # positive sample 선택 (>= pos_iou_thresh IoU)\n",
        "        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n",
        "        pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
        "        if pos_index.size > 0:\n",
        "            pos_index = np.random.choice(\n",
        "                pos_index, size=pos_roi_per_this_image, replace=False)\n",
        "\n",
        "        # Negative sample 선택 [neg_iou_thresh_lo, neg_iou_thresh_hi)\n",
        "        neg_index = np.where((max_iou < self.neg_iou_thresh_hi) &\n",
        "                             (max_iou >= self.neg_iou_thresh_lo))[0]\n",
        "        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image\n",
        "        neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n",
        "                                         neg_index.size))\n",
        "        if neg_index.size > 0:\n",
        "            neg_index = np.random.choice(\n",
        "                neg_index, size=neg_roi_per_this_image, replace=False)\n",
        "\n",
        "        # The indices that we're selecting (both positive and negative).\n",
        "        keep_index = np.append(pos_index, neg_index)\n",
        "        gt_roi_label = gt_roi_label[keep_index]\n",
        "        gt_roi_label[pos_roi_per_this_image:] = 0  # negative sample의 label = 0\n",
        "        sample_roi = roi[keep_index] # (128, 4)\n",
        "\n",
        "        # sample roi와 gt_bbox를 이용해 bbox regression에서 regression해야할 ground truth loc값(t_x, t_y, t_w, t_h) 계산\n",
        "        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]]) # (128, 4)\n",
        "        gt_roi_loc = ((gt_roi_loc - np.array(loc_normalize_mean, np.float32)) / np.array(loc_normalize_std, np.float32))\n",
        "\n",
        "        return sample_roi, gt_roi_loc, gt_roi_label"
      ],
      "metadata": {
        "id": "3xmOxfVKcW29"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training, loss 계산, checkpoint 저장 및 불러오기"
      ],
      "metadata": {
        "id": "aqMdlvZrc3uR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LossTuple = namedtuple('LossTuple', ['rpn_loc_loss', 'rpn_cls_loss',\n",
        "                                     'roi_loc_loss', 'roi_cls_loss',\n",
        "                                     'total_loss'])\n",
        "class FasterRCNNTrainer(nn.Module):\n",
        "\n",
        "    def __init__(self, faster_rcnn):\n",
        "        super(FasterRCNNTrainer, self).__init__()\n",
        "\n",
        "        self.faster_rcnn = faster_rcnn\n",
        "        self.rpn_sigma = rpn_sigma\n",
        "        self.roi_sigma = roi_sigma\n",
        "\n",
        "        # target creator create gt_bbox gt_label etc as training targets. \n",
        "        self.anchor_target_creator = AnchorTargetCreator()\n",
        "        self.proposal_target_creator = ProposalTargetCreator()\n",
        "\n",
        "        self.loc_normalize_mean = faster_rcnn.loc_normalize_mean\n",
        "        self.loc_normalize_std = faster_rcnn.loc_normalize_std\n",
        "\n",
        "        self.optimizer = self.faster_rcnn.get_optimizer()\n",
        "\n",
        "        # training 상태 보여주는 지표\n",
        "        self.rpn_cm = ConfusionMeter(2) # confusion matrix for classification\n",
        "        self.roi_cm = ConfusionMeter(11)  # confusion matrix for classification\n",
        "        self.meters = {k: AverageValueMeter() for k in LossTuple._fields}  # average loss\n",
        "\n",
        "    def forward(self, imgs, bboxes, labels, scale):\n",
        "        n = bboxes.shape[0]\n",
        "        \n",
        "        if n != 1:\n",
        "            raise ValueError('Currently only batch size 1 is supported.')\n",
        "\n",
        "        _, _, H, W = imgs.shape\n",
        "        img_size = (H, W)\n",
        "\n",
        "        # VGG (features extractor)\n",
        "        features = self.faster_rcnn.extractor(imgs)\n",
        "        \n",
        "        # RPN (region proposal)\n",
        "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.faster_rcnn.rpn(features, img_size, scale)\n",
        "\n",
        "        # Since batch size is one, convert variables to singular form\n",
        "        bbox = bboxes[0]\n",
        "        label = labels[0]\n",
        "        rpn_score = rpn_scores[0]\n",
        "        rpn_loc = rpn_locs[0]\n",
        "        roi = rois\n",
        "\n",
        "        \"\"\"\n",
        "        sample roi =  rpn에서 nms 거친 2000개의 roi들 중 positive/negative 비율 고려해 최종 sampling한 roi\n",
        "        \"\"\"\n",
        "        sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(\n",
        "            roi,\n",
        "            tonumpy(bbox),\n",
        "            tonumpy(label),\n",
        "            self.loc_normalize_mean,\n",
        "            self.loc_normalize_std)\n",
        "        \n",
        "        # NOTE it's all zero because now it only support for batch=1 now\n",
        "        # Faster R-CNN head (prediction head)\n",
        "        sample_roi_index = torch.zeros(len(sample_roi))\n",
        "        roi_cls_loc, roi_score = self.faster_rcnn.head(features,sample_roi,sample_roi_index) \n",
        "\n",
        "        # ------------------ RPN losses -------------------#\n",
        "        gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(tonumpy(bbox),anchor,img_size) \n",
        "        gt_rpn_label = totensor(gt_rpn_label).long() \n",
        "        gt_rpn_loc = totensor(gt_rpn_loc) \n",
        "        \n",
        "        # rpn bounding box regression loss\n",
        "        rpn_loc_loss = _fast_rcnn_loc_loss(rpn_loc,gt_rpn_loc,gt_rpn_label.data,self.rpn_sigma)\n",
        "        # rpn classification loss\n",
        "        rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_label.cuda(), ignore_index=-1)\n",
        "        \n",
        "        _gt_rpn_label = gt_rpn_label[gt_rpn_label > -1]\n",
        "        _rpn_score = tonumpy(rpn_score)[tonumpy(gt_rpn_label) > -1]\n",
        "        self.rpn_cm.add(totensor(_rpn_score, False), _gt_rpn_label.data.long())\n",
        "\n",
        "        # ------------------ ROI losses (fast rcnn loss) -------------------#\n",
        "        n_sample = roi_cls_loc.shape[0] \n",
        "        roi_cls_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
        "        roi_loc = roi_cls_loc[torch.arange(0, n_sample).long().cuda(), \\\n",
        "                              totensor(gt_roi_label).long()]\n",
        "        gt_roi_label = totensor(gt_roi_label).long() \n",
        "        gt_roi_loc = totensor(gt_roi_loc) \n",
        "\n",
        "        # faster rcnn bounding box regression loss\n",
        "        roi_loc_loss = _fast_rcnn_loc_loss(\n",
        "            roi_loc.contiguous(),\n",
        "            gt_roi_loc,\n",
        "            gt_roi_label.data,\n",
        "            self.roi_sigma)\n",
        "\n",
        "        # faster rcnn classification loss\n",
        "        roi_cls_loss = nn.CrossEntropyLoss()(roi_score, gt_roi_label.cuda())\n",
        "        \n",
        "        self.roi_cm.add(totensor(roi_score, False), gt_roi_label.data.long())\n",
        "\n",
        "        losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]\n",
        "        losses = losses + [sum(losses)] # total_loss == sum(losses)\n",
        "\n",
        "        return LossTuple(*losses)\n",
        "    \n",
        "    # training\n",
        "    def train_step(self, imgs, bboxes, labels, scale):\n",
        "        self.optimizer.zero_grad()\n",
        "        losses = self.forward(imgs, bboxes, labels, scale)\n",
        "        losses.total_loss.backward() # training sector에서 backward 진행 -> roi와 rpn의 weight를 update해줌\n",
        "        self.optimizer.step()\n",
        "        self.update_meters(losses)\n",
        "        return losses\n",
        "    \n",
        "    # checkpoint 만들기\n",
        "    def save(self, save_optimizer=False, save_path=None):\n",
        "        save_dict = dict()\n",
        "\n",
        "        save_dict['model'] = self.faster_rcnn.state_dict()\n",
        "\n",
        "        if save_optimizer:\n",
        "            save_dict['optimizer'] = self.optimizer.state_dict()\n",
        "\n",
        "        if save_path is None:\n",
        "            save_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth'\n",
        "\n",
        "        save_dir = os.path.dirname(save_path)\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        torch.save(save_dict, save_path)\n",
        "        return save_path\n",
        "    \n",
        "    # checkpoint load\n",
        "    def load(self, path, load_optimizer=True, parse_opt=False, ):\n",
        "        state_dict = torch.load(path)\n",
        "        if 'model' in state_dict:\n",
        "            self.faster_rcnn.load_state_dict(state_dict['model'])\n",
        "        else:  # legacy way, for backward compatibility\n",
        "            self.faster_rcnn.load_state_dict(state_dict)\n",
        "            return self\n",
        "        if 'optimizer' in state_dict and load_optimizer:\n",
        "            self.optimizer.load_state_dict(state_dict['optimizer'])\n",
        "        return self\n",
        "\n",
        "    def update_meters(self, losses):\n",
        "        loss_d = {k: scalar(v) for k, v in losses._asdict().items()}\n",
        "        for key, meter in self.meters.items():\n",
        "            meter.add(loss_d[key])\n",
        "\n",
        "    def reset_meters(self):\n",
        "        for key, meter in self.meters.items():\n",
        "            meter.reset()\n",
        "        self.roi_cm.reset()\n",
        "        self.rpn_cm.reset()\n",
        "\n",
        "    def get_meter_data(self):\n",
        "        return {k: v.value()[0] for k, v in self.meters.items()}\n",
        "\n",
        "\n",
        "def _smooth_l1_loss(x, t, in_weight, sigma):\n",
        "    sigma2 = sigma ** 2\n",
        "    diff = in_weight * (x - t)\n",
        "    abs_diff = diff.abs()\n",
        "    flag = (abs_diff.data < (1. / sigma2)).float()\n",
        "    y = (flag * (sigma2 / 2.) * (diff ** 2) +\n",
        "         (1 - flag) * (abs_diff - 0.5 / sigma2))\n",
        "    return y.sum()\n",
        "\n",
        "\n",
        "def _fast_rcnn_loc_loss(pred_loc, gt_loc, gt_label, sigma):\n",
        "    # Localization loss 구할 때는 positive example에 대해서만 계산\n",
        "    in_weight = torch.zeros(gt_loc.shape).cuda()\n",
        "    in_weight[(gt_label > 0).view(-1, 1).expand_as(in_weight).cuda()] = 1\n",
        "    loc_loss = _smooth_l1_loss(pred_loc, gt_loc, in_weight.detach(), sigma)\n",
        "    loc_loss /= ((gt_label >= 0).sum().float())\n",
        "    return loc_loss"
      ],
      "metadata": {
        "id": "iOGUI44uctVE"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train 진행\n",
        "- 데이터셋 불러오고 trainer 불러오고, trainer step 과정 거치고, faster_rcnn에 대해 학습 진행"
      ],
      "metadata": {
        "id": "oOSqEdkjmycB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # Train dataset 불러오기\n",
        "#     dataset = TrainDataset()\n",
        "    annotation = os.path.join(data_dir,'train.json')\n",
        "    dataset = TrainCustom(annotation, data_dir, transforms=True)\n",
        "    print('load data')\n",
        "    dataloader = data_.DataLoader(dataset, \n",
        "                                  batch_size=1,     # only batch_size=1 support\n",
        "                                  shuffle=True, \n",
        "                                  pin_memory=False,\n",
        "                                  num_workers=4)\n",
        "    \n",
        "    # faster rcnn 불러오기\n",
        "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
        "    print('model construct completed')\n",
        "    \n",
        "    # faster rcnn trainer 불러오기\n",
        "    trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n",
        "    \n",
        "    # checkpoint load\n",
        "    if train_load_path:\n",
        "        trainer.load(train_load_path)\n",
        "        print('load pretrained model from %s' % train_load_path)\n",
        "    \n",
        "    lr_ = learning_rate\n",
        "    best_loss = 1000\n",
        "    for epoch in range(epochs):\n",
        "        trainer.reset_meters()\n",
        "        for ii, (img, bbox_, label_, scale) in enumerate(tqdm(dataloader)):\n",
        "            \n",
        "            img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n",
        "            trainer.train_step(img, bbox, label, float(scale))\n",
        "        \n",
        "        losses = trainer.get_meter_data()\n",
        "        print(f\"Epoch #{epoch+1} loss: {losses}\")\n",
        "        if losses['total_loss'] < best_loss :\n",
        "            trainer.save()\n",
        "            \n",
        "        if epoch == 9:\n",
        "            trainer.faster_rcnn.scale_lr(lr_decay)\n",
        "            lr_ = lr_ * lr_decay\n",
        "\n",
        "        if epoch == 13: \n",
        "            break"
      ],
      "metadata": {
        "id": "fzOGD0IVeP-g"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "RTonMt2_ncFn",
        "outputId": "8d739292-8993-4e5a-d939-4dc243780fbd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-2da0ffaf5447>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-e073ff59090a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     dataset = TrainDataset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainCustom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     dataloader = data_.DataLoader(dataset, \n",
            "\u001b[0;32m<ipython-input-42-a4a34d2f75ee>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation, data_dir, transforms)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# coco annotation 불러오기 (coco API)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading annotations into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/data/train.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qHE8vgqQne8j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}